import os
from typing import Dict
from typing import List
from urllib.parse import urlencode

import bs4
import requests
import streamlit as st
from dotenv import load_dotenv
from langchain.embeddings import CacheBackedEmbeddings
from langchain.retrievers import EnsembleRetriever
from langchain.storage import LocalFileStore
from langchain_chroma import Chroma
from langchain_community.document_loaders import WebBaseLoader
from langchain_community.retrievers import BM25Retriever
from langchain_core.documents import Document
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.vectorstores import VectorStore
from langchain_ollama import OllamaEmbeddings, ChatOllama
from langchain_text_splitters import RecursiveCharacterTextSplitter


class NaverOpenAPIClient:
    """
    https://developers.naver.com/docs/serviceapi/search/news/news.md#%EB%89%B4%EC%8A%A4

    client = NaverOpenAPIClient(
        client_id=os.getenv("NAVER_OPEN_API_CLIENT_ID"),
        client_secret=os.getenv("NAVER_OPEN_API_CLIENT_SECRET"),
    )
    news_urls = client.get_news_urls(
        query="ê³„ì—„ë ¹",
        display=10,
    )
    """

    def __init__(
        self,
        client_id: str,
        client_secret: str,
        api_url: str = "https://openapi.naver.com",
    ):
        self.client_id = client_id
        self.client_secret = client_secret
        self.api_url = api_url

    @property
    def header(self) -> Dict[str, str]:
        return {
            "X-Naver-Client-Id": self.client_id,
            "X-Naver-Client-Secret": self.client_secret,
        }

    def http_get(self, url: str) -> Dict:
        response = requests.get(url, headers=self.header)
        response.raise_for_status()
        return response.json()

    def request_news(
        self,
        query: str,
        display: int,
        start: int,
        sort: str,
    ) -> Dict:
        assert query != ""
        assert 10 <= display <= 100
        assert 1 <= start <= 100
        assert sort in ["sim", "date"]

        params = urlencode({"query": query, "display": display, "start": start, "sort": sort})
        url = f"{self.api_url}/v1/search/news.json?{params}"
        return self.http_get(url)

    def get_news_urls(
        self,
        query: str,
        display: int = 10,
        start: int = 1,
        sort: str = "date",
    ) -> List[str]:
        news_items = self.request_news(query, display, start, sort)
        return [item["link"] for item in news_items["items"]]


load_dotenv()
# set_debug(True)
# logger = logging.getLogger(__name__)
# logger.setLevel(logging.DEBUG)
# logger.addHandler(logging.StreamHandler())


# @cacheable(cache_loader=RedisCacheLoader)
def get_news_urls_by_keyword(
    keyword: str,
    display: int = 10,
    start: int = 1,
    sort: str = "date",
) -> List[str]:
    # logger.debug("get_news_urls_by_keyword...")
    client = NaverOpenAPIClient(
        client_id=os.getenv("NAVER_OPEN_API_CLIENT_ID"),
        client_secret=os.getenv("NAVER_OPEN_API_CLIENT_SECRET"),
    )
    urls = client.get_news_urls(
        query=keyword,
        display=display,
        start=start,
        sort=sort,
    )
    # logger.debug(urls)
    return urls


def load_documents(urls: List[str]) -> List[Document]:
    # logger.debug("load_documents...")
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=100,
        chunk_overlap=0,
        add_start_index=True,
        length_function=len,
        is_separator_regex=False,
        separators=["\n", "\n\n"],
    )
    naver_news_loader = WebBaseLoader(
        web_paths=list(filter(lambda x: "https://n.news.naver.com" in x, urls)),
        bs_kwargs=dict(
            parse_only=bs4.SoupStrainer("div", attrs={"id": ["newsct_article"]}),
        ),
    )
    another_news_loader = WebBaseLoader(
        web_paths=list(filter(lambda x: "https://n.news.naver.com" not in x, urls)),
    )

    def escape_content(text: str) -> str:
        return text.strip().replace("\t", "").replace("\n", "")

    documents = []
    for doc in naver_news_loader.load() + another_news_loader.load():
        doc.page_content = escape_content(doc.page_content)
        documents.append(doc)

    return text_splitter.split_documents(documents)


def create_vector_db(documents: List[Document]) -> Chroma:
    # logger.debug("create_vector_db...")
    chroma_path = ".vector.summary"
    store_path = ".store.summary"

    embeddings = OllamaEmbeddings(model="bge-m3")
    cached_embeddings = CacheBackedEmbeddings.from_bytes_store(
        underlying_embeddings=embeddings,
        document_embedding_cache=LocalFileStore(store_path),
        namespace=embeddings.model,
    )
    return Chroma.from_documents(
        documents=documents,
        embedding=cached_embeddings,
        persist_directory=chroma_path,
    )


def create_retriever(vector_db: VectorStore, documents: List[Document]) -> EnsembleRetriever:
    # logger.debug("create_retriever...")
    bm25_retriever = BM25Retriever.from_documents(
        documents=documents,
        search_kwargs={
            "k": 5,
        },
    )
    vector_retriever = vector_db.as_retriever(
        search_type="mmr",
        search_kwargs={"k": 5},
    )
    return EnsembleRetriever(
        retrievers=[bm25_retriever, vector_retriever],
        weights=[0.5, 0.5],
    )


def flatten_documents(documents: List[Document]):
    return "\n\n".join(doc.page_content for doc in documents)


def create_prompt_for_news() -> ChatPromptTemplate:
    template = """
ë‚˜ëŠ” ë‹¹ì‹ ì´ **ë‰´ìŠ¤ ìš”ì•½ ì „ë¬¸ê°€**ë¡œì„œ í–‰ë™í•˜ê¸°ë¥¼ ë°”ëë‹ˆë‹¤. ë‚˜ëŠ” ë‹¹ì‹ ì—ê²Œ ë‰´ìŠ¤ ë‚´ìš©ë“¤ì„ ì œê³µí•  ê²ƒ ì…ë‹ˆë‹¤.   
ë‹¹ì‹ ì€ ì•„ë˜ ì§€ì¹¨ì„ ë”°ë¼ ìš”ì•½ ë‚´ìš©ì„ ì œê³µ í•˜ì„¸ìš”.
1. Newsì—ì„œë§Œ ì œê³µëœ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìš”ì•½í•˜ì„¸ìš”. ì™¸ë¶€ì˜ ì¶”ê°€ ì •ë³´, ë³¸ì¸ì˜ ìƒê°, ìƒì‹ ë“±ì„ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.
2. ë‚´ìš©ì€ ê°ê´€ì ì´ê³  ì¤‘ë¦½ì ì¸ ì‹œê°ì—ì„œ ì‘ì„±í•˜ì„¸ìš”.
3. ë‰´ìŠ¤ì— ì–¸ê¸‰ëœ ìˆ«ì, í†µê³„, ì¸ìš©ë¬¸ ë“± ì¤‘ìš”í•œ ì„¸ë¶€ ì •ë³´ë¥¼ ë°˜ë“œì‹œ í¬í•¨í•˜ì„¸ìš”.
4. ì£¼ì œì™€ ê´€ë ¨ ì—†ëŠ” ì„¸ë¶€ ì‚¬í•­ì´ë‚˜ ë¶€ì°¨ì ì¸ ì •ë³´ë¥¼ ì œê±°í•˜ê³  í•µì‹¬ì ì¸ ë‚´ìš©ì„ ê°„ê²°í•˜ê²Œ ì „ë‹¬í•˜ì„¸ìš”.
5. ë‰´ìŠ¤ì˜ ì¶œì²˜ê°€ ëª…í™•í•˜ì§€ ì•Šì€ ì •ë³´ë‚˜ ì¶”ì¸¡ì„± ë‚´ìš©ì€ ìš”ì•½ì— í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.

# ë‰´ìŠ¤: 
{articles}

# ì‘ë‹µì€ ë°˜ë“œì‹œ ì•„ë˜ í˜•ì‹ì„ ë”°ë¼ ì£¼ì„¸ìš”:
ìš”ì•½: ì „ì²´ ë‚´ìš©ì— ëŒ€í•œ ì£¼ì œ (50ë‹¨ì–´ ì´ë‚´)
ë‚´ìš©: ë‚´ìš© ìš”ì•½ (200ë‹¨ì–´ ì´ë‚´) 
í•µì‹¬ ë‚´ìš©:
- í•µì‹¬ ë‚´ìš© 1 (30ë‹¨ì–´ ì´ë‚´) (source)
- í•µì‹¬ ë‚´ìš© 2 (30ë‹¨ì–´ ì´ë‚´) (source)
- í•µì‹¬ ë‚´ìš© 3 (30ë‹¨ì–´ ì´ë‚´) (source)
- ...

# ì§ˆë¬¸:
{question}
    """.strip()
    return ChatPromptTemplate.from_template(template)


def app_main():
    st.title("ğŸ’¬ Langchain News")

    form = st.form("my_form")
    keyword = form.text_input("ê²€ìƒ‰ í•˜ê³  ì‹¶ì€ ë‰´ìŠ¤ í‚¤ì›Œë“œ (ex, ê³„ì—„ë ¹):")
    user_input = form.text_input("ê²€ìƒ‰í•œ ë‰´ìŠ¤ì— ëŒ€í•´ì„œ ê¶ê¸ˆí•˜ì‹ ê±¸ ì§ˆë¬¸í•´ì£¼ì„¸ìš” (ex, ê³„ì—„ë ¹ì´ ì½”ìŠ¤í”¼ì— ë¯¸ì¹œ ì˜í–¥):")
    submitted = form.form_submit_button("Submit")

    if keyword and user_input and submitted:
        print(keyword, " / ", user_input)
        new_urls = get_news_urls_by_keyword(keyword, display=20)
        documents = load_documents(new_urls)
        vector_db = create_vector_db(documents)
        retriever = create_retriever(vector_db, documents)

        llm = ChatOllama(
            model="exaone3.5:7.8b",
            temparature=0,
        )
        prompt = create_prompt_for_news()
        chain = {"articles": retriever, "question": RunnablePassthrough()} | prompt | llm | StrOutputParser()

        output = chain.invoke(user_input)
        st.write(output)


if __name__ == '__main__':
    app_main()
